{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7b5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3828ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BASELINE: Convolutional Autoencoder\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# 2. MEDICAL STANDARD: U-Net\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(1, 64, 3, padding=1), nn.ReLU())\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(nn.Conv2d(128, 256, 3, padding=1), nn.ReLU())\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec1 = nn.Sequential(nn.Conv2d(256, 128, 3, padding=1), nn.ReLU()) # 256 due to concat\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1), nn.ReLU())\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool(e1)\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool(e2)\n",
    "        \n",
    "        b = self.bottleneck(p2)\n",
    "        \n",
    "        u1 = self.up1(b)\n",
    "        cat1 = torch.cat((u1, e2), dim=1) # Skip Connection\n",
    "        d1 = self.dec1(cat1)\n",
    "        \n",
    "        u2 = self.up2(d1)\n",
    "        cat2 = torch.cat((u2, e1), dim=1) # Skip Connection\n",
    "        d2 = self.dec2(cat2)\n",
    "        \n",
    "        return torch.sigmoid(self.final(d2))\n",
    "\n",
    "# 3. GENERATIVE: Variational Autoencoder\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.enc1 = nn.Conv2d(1, 32, 4, stride=2, padding=1)\n",
    "        self.enc2 = nn.Conv2d(32, 64, 4, stride=2, padding=1)\n",
    "        self.enc3 = nn.Conv2d(64, 128, 4, stride=2, padding=1)\n",
    "        \n",
    "        # Flatten size depends on input (assuming 128x128 -> 16x16 feature map)\n",
    "        self.fc_mu = nn.Linear(128*16*16, 512)\n",
    "        self.fc_logvar = nn.Linear(128*16*16, 512)\n",
    "        self.decoder_input = nn.Linear(512, 128*16*16)\n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
    "        self.dec2 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)\n",
    "        self.dec3 = nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        \n",
    "        mu, logvar = self.fc_mu(x_flat), self.fc_logvar(x_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        z = self.decoder_input(z).view(-1, 128, 16, 16)\n",
    "        z = F.relu(self.dec1(z))\n",
    "        z = F.relu(self.dec2(z))\n",
    "        recon = torch.sigmoid(self.dec3(z))\n",
    "        return recon, mu, logvar\n",
    "\n",
    "\n",
    "# 4. ADVANCED: Residual Autoencoder (ResNet-AE)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + self.conv_block(x)) # Element-wise Sum (The \"Residual\" part)\n",
    "\n",
    "class ResNetAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.start = nn.Sequential(nn.Conv2d(1, 64, 3, padding=1), nn.ReLU())\n",
    "        self.res1 = ResidualBlock(64)\n",
    "        self.down1 = nn.Conv2d(64, 128, 3, stride=2, padding=1) # Down\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.down2 = nn.Conv2d(128, 256, 3, stride=2, padding=1) # Down\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.res3 = ResidualBlock(256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.res5 = ResidualBlock(64)\n",
    "        self.final = nn.Sequential(nn.Conv2d(64, 1, 3, padding=1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.down1(self.res1(x))\n",
    "        x = self.down2(self.res2(x))\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(self.up1(x))\n",
    "        x = self.final(self.res5(self.up2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c26d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07726454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
